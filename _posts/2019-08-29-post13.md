---
title: "단어 분리(Subword Segmentation)"
categories:
  - 지식창고
tags:
  - Subword Segmentation
  - BPE
  - WPM
  - NLP
---

## BPE(Byte Pair Encoding)

BPE(Byte Pair Encoding)은 [**Neural Machine Translation of Rare Words with Subword Units**](https://arxiv.org/abs/1508.07909) 논문에서 소개된 자연어 처리 방법이다.

1. 데이터에서 단어들의 빈도수를 계산한다.
2. 모든 단어들을 글자 단위(유니그램)로 나눈다.
3. 가장 빈도수가 높은 유니그램 쌍을 하나의 유니그램으로 통합한다.
4. 3번을 반복한다.

### 예시

데이터로부터 단어들의 사전을 만들고 각 단어의 빈도 수를 계산한다. 

여기서, 기존 단어들의 끝에는 `\<w>` 토큰을 붙인다.

```
# 단어와 빈도 수
low<\w>: 5
lower<\w>: 2
newest<\w>: 6
widest<\w>: 3
```

모든 단어들을 글자 단위(유니그램)으로 나눈다.

```
# 단어들을 유니그램으로 나눔
l o w <\w>: 5
l o w e r <\w>: 2
n e w e s t <\w>: 6
w i d e s t <\w>: 3
```

```
# 단어 사전
l, o, w, <\w>, e, r, n, w, s, t, i, d
```

빈도 수가 가장 높은 유니그램 쌍을 하나의 유니그램으로 통합한다.

여기서는 빈도 수가 9로 가장 높은 (e, s) 쌍을 es로 통합한다.

```
# 빈도 수가 가장 높은 유니그램 쌍을 통합
l o w <\w>: 5
l o w e r <\w>: 2
n e w es t <\w>: 6
w i d es t <\w>: 3
```

```
# 단어 사전
l, o, w, <\w>, e, r, n, w, es, t, i, d
```

다음으로 빈도 수가 9로 가장 높은 (es, t) 쌍을 est로 통합한다.

```
# 빈도 수가 가장 높은 유니그램 쌍을 통합
l o w <\w>: 5
l o w e r <\w>: 2
n e w est <\w>: 6
w i d est <\w>: 3
```

```
# 단어 사전
l, o, w, <\w>, e, r, n, w, est, i, d
```

위의 과정을 계속 반복한다.

같은 과정을 총 10번 반복하면 아래와 같은 결과를 얻을 수 있다.

```
# 10번 반복한 결과
low<\w>: 5
low e r <\w>: 2
newest<\w>: 6
wi d est<\w>: 3
```

```
# 단어 사전
low<\w>, low, e, r, <\w>, newest<\w>, wi, d, est<\w>
```

이제, `lowest`라는 단어가 등장하게 되면 이 단어는 더 이상 OOV 문제를 일으키지 않는다.

먼저, `lowest`를 `l, o, w, e, s, t`로 나눈 후에 단어 사전에서 `low`와 `est<\w>`를 찾아서 `lowest`를 `low`, `est<\w>` 두 단어로 인코딩한다.

이처럼, BPE를 사용하면 OOV 문제를 해결할 수 있고 효율적으로 단어를 인코딩할 수 있다.

## WPM(Word Piece Model)

WPM(Word Piece Model)은 [**Japanese and Korean voice search[1]**](https://ieeexplore.ieee.org/abstract/document/6289079) 논문과 [**Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation[2]**](https://arxiv.org/abs/1609.08144) 논문에서 소개된 자연어 처리 방법이다.

WPM은  BPE와 비슷하지만 유니그램을 빈도 수를 기준으로 통합하는 것이 아니라 Language Model의 likelihood를 최대화하는 방향으로 유니그램을 통합한다.

1. 데이터에서 단어들을 유니코드 글자 단위로 나눠 단어 사전을 만든다.
2. 1에서 만든 사전과 데이터를 기반으로 Language Model을 만든다.
3. 단어 사전의 글자들을 합쳐 사전에 없는 새로운 단어를 하나 만들고, 새로운 사전으로 Language Model의 likelihood를 측정해, likelihood를 증가시키는 새로운 단어를 선택해 사전에 추가한다.
4. 미리 정의해둔 단어 사전의 수에 도달하거나, likelihood가 증가하는 정도가 threshold 값 보다 내려갈 때 까지 2~3 과정을 반복한다.

### 예시

**Word** `Jet makers feud over seat width with big orders at stake `

**WPM** `_J et _makers _fe ud _over _seat _width _with _big _orders _at _stake`

WPM은 기존 문장의 단어들을 subword들로 나눌 수 있다.

사전에 `Jet`라는 단어가 존재하지 않기 때문에 `J`와 `et`로 나누어진다.

하지만 그냥 `J`와 `et`로 나눠버리면 원래 `J`와 `et` 두 단어 였는지, `Jet`가 두 단어로 나눠진 것인지 구분할 수  없다.

이런 문제를 해결하기 위해 기존 문장의 모든 단어 앞에 `_`를 붙이고 subword로 나눠져 새로 생긴 단어의 앞에는 추가로 `_`를 붙이지 않는다.

WPM을 적용한 토큰들의 공백을 제거해서 모든 토큰들을 이어 붙인 후에, `_`를 공백으로 치환하면 기존 문장을 복원할 수 있다.

## Mixed Word / Character Model

WPM과 마찬가지로   [**Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation[2]**](https://arxiv.org/abs/1609.08144) 논문에서 소개된 방법이다.

일반적으로 정해진 단어 사전에 등장하지 않는 단어들은 모두 `<UNK>`토큰으로 치환한다.

하지만, [2] 논문에서는 모든 OOV 단어들을 똑같이 `<UNK>`로 치환하는 것이 아니라 글자 단위로 나눈다.

여기서, 그냥 글자 단위로 나누는게 아니라 글자 앞에 추가적으로 `<B>`, `<M>`, `<E>` 토큰들을 붙인다.

`Miki`라는 단어가 사전에 존재하지 않는 경우, 이를 `M`, `i`, `k`, `i`로 나눈다.

그 후에 각 글자들의 앞에 토큰을 붙여 `<B>M`, `<M>i`, `<M>k`, `<E>i`로 만든다.

토큰들을 제거하면서 나눠진 글자들을 이어 붙이면 기존 단어를 복원할 수 있다.