---
title: "Decoding methods"
categories:
  - 지식창고
tags:
  - Decoding
  - NLP
---

이번 포스팅에서는 languge generation의 여러 decoding method들에 대해 알아보고자 한다.

Decoding은 모델이 연산을 거쳐 만든 vocabulary space로 표현된 vector 또는 tensor를 하나의 단어 또는 토큰으로 매핑시키는 과정이다.

쉽게 말하면 embedding의 반대라고 볼 수 있다.

Embedding이 index로 표현된 단어들을 hidden state로 매핑시킨다면, decoding은 hidden state를 vocabulary size로 만든 후에 단어의 index로 매핑시킨다.



## Greedy decoding

가장 기본적인 방법이다.

말 그대로 greedy 방식으로 단어를 만든다.

매 time step 마다 단순하게 확률이 가장 높은 단어로 매핑시킨다.

Vocabulary size의 output에 argmax 연산을 적용해 가장 확률이 높은 단어를 뽑을 수 있다.

Greedy decoding은 단어를 뽑을 때 문장 전체의 확률을 고려하지 않는다.

즉, 각각의 time step에서 가장 확률이 높은 단어들을 뽑지만, 전체 문장의 관점에서 보면 최적의 확률이 보장되지 않는다.



## Beam decoding

Greedy decoding의 단점을 어느정도 보완할 수 있는 방법이다.

Beam의 크기 $$k$$를 미리 정의하고 매 time step 마다 확률이 높은 순서대로 $$k$$개의 단어를 뽑아 beam을 갱신한다.

즉, 매 time step 마다 확률이 높은 $$k$$개의 경로를 추적한다.

$$k$$개의 beam을 유지함으로써, 지금 당장은 가장 높은 확률의 단어가 아니더라도, 전체 문장의 관점에서 보면 더 높은 확률의 문장을 만들 수 있다.



## Sampling

위의 decoding 방식들은 모델이 계산한 확률을 100% 신뢰하여 가장 확률이 높은 단어를 뽑는 방식이다.

하지만, sampling은 모델이 계산한 확률을 하나의 분포로 간주하여 단어를 무작위로 뽑는다.

즉, 확률이 낮게 계산된 단어라도 뽑힐 가능성이 생긴다.

Sampling을 사용하면 좀 더 유연하고 다양한 문장을 생성할 수 있다.



## Top-k sampling

Top-k sampling은 단어의 분포에 제한을 둠으로써, 확률이 낮은 단어들을 배제하고 sampling 하는 방식이다.

확률이 높은 순으로 k개의 단어들을 추출해서 새로운 분포를 만들고, 그 분포에서 단어를 sampling 한다.

K개의 단어로 제한을 둠으로써, 엉뚱한 단어를 생성할 가능성을 배제시킬 수 있다.



## Nucleus sampling(top-p sampling)

Top-k sampling과 비슷하게 단어의 분포에 제한을 두는 방식이다.

단, top-k sampling이 단어의 개수만 고려한다면 nucleus sampling은 추출하는 단어들의 확률까지 고려한다.

만약, k를 30으로 정해 단어를 추출했는데 확률이 높은 순서대로 10개 까지의 단어들의 누적 확률이 100%에 가깝다면, 남은 20개의 단어들은 전혀 엉뚱한 단어가 될 수 있다.

즉, top-k sampling이 의도대로 작동하지 않게 된다.

이러한 단점을 보완하기 위해 k개의 단어들의 누적 확률을 고려한다.

먼저, 확률이 높은 순서대로 k개의 단어들을 추출한다.

그 후, 확률이 높은 순서대로 누적 확률이 미리 정의해둔 p를 넘어가지 않도록 확률이 높은 단어들만 다시 추출한다.

이렇게 추출된 단어들로 만든 확률 분포에서 단어를 sampling한다.