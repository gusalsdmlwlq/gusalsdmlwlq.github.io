---
title: "Language Models are Unsupervised Multitask Learners(GPT-2)"
categories:
  - 논문리뷰
tags:
  - GPT
  - Language Model
  - Transformer
  - NLP
  - Deep Learning
  - Attention
---

Question answering. machine translation, reading comprehension and summarization 등의 NLP task들은 보통 해당 task에 맞는 특정 dataset에서 supervised 방식으로 학습된다.

본 연구에서는 language model이 이러한 task들을  WebText라는 큰 dataset에서 supervision 없이 학습할 수 있다는 것을 설명한다.

본 연구는 학습 데이터에 label을 만들지 않고 많은 task를 수행할 수 있는 좀 더 일반적인 시스템을 만드는 것을 목적으로 한다.

이런 일반적인 시스템을 만들기 위해서는 다양한 domain과 task에서 학습되고 성능을 측정할 필요가 있다.

Multitask learning이 이런 시스템의 성능을 향상시켜 줄 것이다.

Language task의 시스템은 아래와 같은 변화를 겪어왔다.

1. Word vector를 학습해서 task specific한 구조에 입력하는 시스템
2. Recurrent한 구조를 이용해 contextual representation을 transfer하는 시스템
3. Task specific한 구조 없이 많은 self attention block들을 transfer하는 시스템

본 연구에서는 language model이 zero shot에서 down stream task들을 수행할 수 있다는 것을 설명한다.

# Approach

본 연구의 핵심은 language modeling이다.

즉, 기본적으로 다음 단어를 예측하는 것이다.

하나의 task를 수행하기 위해 학습되는 모델은 conditional probabilty $$p(output\mid input)$$을 계산하면 된다.

하지만, 좀 더 일반적인 시스템은 다양한 task를 수행해야 하기 때문에 conditional probability $$p(output\mid input,task)$$를 계산해야 한다.

Language는 $$(task,input,output)$$를 다양하게 정의할 수 있다.

예를 들어, translation의 경우는 `(translate to french, english text, french text)`가 될 수 있고 reading comprehension의 경우는 `(answer the question, document, question, answer)`가 될 수 있다.

Language modeling에서는 어떤 symbol이 출력되어야 하는지에 대한 supervision이 필요가 없다.

거대한 language model은 multitask를 수행할 수 있지만 supervised learning에 비해 속도가 많이 느리다.

## Dataset

기존의 연구들은 대부분 하나의 domain의 text에 대해서 학습을 시켜왔다.

본 연구에서는 다양한 domain의 natural language를 모으기 위해 크고 다양한 dataset를 사용했다.

다양한 데이터를 수집하기 위해 web crawling을 사용했다.

하지만 web 데이터는 질이 떨어지는 text가 많다는 문제가 있기 때문에 social media platform인 Rddit에서 데이터를 수집할 때, 최소 3개의 karma를 받은 데이터만 수집했다.

또한 Wikipedia 문서는 다른 dataset의 공통이 되는 source이기 때문에 보다 정확한 evaluation을 위해서 dataset에서 제외했다.

## Input Representation

일반적인 language model은 어떤 문자열이든 확률을 계산할 수 있어야 한다.

현재의 거대한 language model은 lower casting, tokenization, OOV 등의 prepreocessing을 포함하고 있다.

Unicode 문자열을 UTF-8 sequence로 처리하는 것은 이러한 조건을 만족하지만, 현재 byte level의 language model은 word level의 language 모델에 비해 경쟁력이 없다.

BPE는 자주 발생하는 symbol sequence는 word level로, 그렇지 않은 symbol은 character level로 input을 만들기 때문에 character level과 word level의 중간 이라고 할 수 있다.

하지만, BPE를 구현함에 있어 종종 byte의 sequence가 아닌 Unicode로 동작하는 경우가 있다.

이런 방법은 모든 Unicode symbol을 저장해야하는 문제점이 있다.

반대로, BPE를 byte sequence에 바로 적용시키면 greedy 방식을 사용하는 BPE의 특성상 optimal하지 않게 작동하는 문제가 생길 수 있다.

예를 들면, `dog.`, `dog!`, `dog?`처럼 `dog`라는 같은 단어가 vocabulary안에 여러개 생길 수 있다.

이런 문제를 해결하기 위해 본 연구에서는  byte sequence가 character 단위와 merge되는 것을 막았다.

또한, 공백에 대한 예외를 추가해서 압축의 효율을 향상시켰고 여러 vocabulary token들에서 단어의 fragmentation을 최소화시켰다.

이러한 방법은 모든 Unicode 문자열의 확률을 계산할 수 있기 때문에 추가적인 preprocessing 과정 없이 language model을 평가할 수 있다.

## Model

Transformer 기반의 language model을 사용한다.

기본적으로 GPT 모델과 비슷하며 조금 변형시켰다.

Layer normalization을 각 sub block의 시작 부분으로 옮겼고 마지막 self attention block의 뒤에 layer normalization을 추가했다.

Residual path에서 모델의 깊이의 누적을 계산하도록 변형된 초기화를 사용한다.

초기화할 때, residual layer를 $$\frac{1}{\sqrt{N}}$$으로 scaling한다. ($$N$$은 layer의 수) 

Context size도 512에서 1024로 늘리고 batch size를 512로 설정했다.

## 논문 링크

[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

