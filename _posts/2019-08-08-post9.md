---
title: "Attention Is All You Need"
categories:
  - 논문리뷰
tags:
  - Attention
  - Transformer
  - NLP
  - Deep Learning
---

## Attention

기존의 RNN(Recurrent Neural Network)는 길이가 긴 sequence에서 멀리 떨어진 token에 대한 dependency를 만드는 데 문제가 있다.

이를 보완하기 위해 LSTM(Long Short Term Memory)과 GRU(Gated Recurrent Unit)을 개발했다.

이 모델들은 gate를 사용해 멀리 떨어진 token에 대한 dependency를 어느정도 만들 수 있었으나 역시나 문제를 완전히 해결하지는 못했다.

이런 문제를 해결하기 위해 attention이라는 개념이 만들어졌다.

Attention을 사용하면 token의 거리에 관계 없이 dependency를 만들 수 있다.

### Self attention

본 논문에서는 self attention이라는 개념을 소개한다.

기존의 attention을 사용한 RNN 모델들은 encoder와 decoder사이에 attention layer를 두고 encoder에서 중요한 부분들을 기억하고 그 정보를 decoder에 전달하는 역할을 수행했다.

Self attention은 기존의 attention과는 다르게 encoder 또는 decoder 내에서 하나의 sequence의 서로 다른 위치의 token들의 관계를 계산함으로써 그 sequence의 representation을 구한다.

## Transformer

본 논문에서는 transformer라는 새로운 모델을 소개한다.

RNN과 같은 recurrent한 구조를 사용하지 않고 attention만 사용해서 입력과 출력 사이의 dependency를 만든다.

먼저, encoder는 입력 sequence, $$X=[x_1, x_2, ..., x_n]$$을 연속적인 sequence, $$Z=[z_1, z_2, ..., z_n]$$로 변환한다.

Decoder는 $$Z$$가 주어지면 출력 sequence, $$Y=[y_1, y_2, ..., y_m]$$을 생성한다.

매 step마다, 이전에 생성된 출력을 추가적인 입력으로 사용해 다음 step의 출력을 생성한다.

Encoder와 decoder 모두 stacked self attention과 position-wise FC layer로 구성된다.

Transformer 모델의 구조는 아래와 같다.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer1.png" alt="">
  <figcaption style="font-size:12px">출처:http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</figcaption>
</figure> 

### 1. Encoder

6개의 같은 layer가 stack된 구조이다.

각 layer는 multi-head self attention과 position-wise FC feed forward network, 두개의 sub layer로 구성된다.

각 sub layer는 residual connection을 가진다.

즉, sub layer의 입력이 $$x$$라고 할 때, $$Norm(x+Sublayer(x))$$와 같은 결과를 만든다.

### 2. Decoder

Encoder와 마찬가지로 6개의 같은 layer가 stack된 구조이다.

Encoder와 다른 점은 encoder의 2개의 sub layer 외에도 encoder의 출력에 대한 multi-head attention이 존재한다.

즉, decoder의 각 layer는 encoder와는 다르게 3개의 sub layer로 구성된다.

Decoder의 sub layer들 역시 residual connection을 가진다.

Encoder의 multi-head self attention과는 다르게 decoder에서는 masking을 사용해 뒤의 token들에 관여하지 못하게 한다.

즉, $$i$$번째 position에서는 $$i$$번째 이전까지의 정보들만 사용할 수 있게 된다.

Decoder는 예측을 통해 출력을 만들어내기 때문에 학습 과정에서 미래의 출력 정보들을 사용하지 못하게 방지한다고 생각하면 될 것 같다.

### 3. Scaled Dot Product Attention

본 논문에서는 transformer 모델에서 사용하는 attention에 scaled dot product attention이라는 이름을 붙였다.

여기서는 Query, Key, Value라는 개념을 사용한다.

Query와 Key는 $$d_k$$차원이고 Value는 $$d_v$$차원의 vector이다.

Transformer의 attention에서는 위 3개의 vector들의 연산을 통해 출력을 얻는다.

계산 과정은 아래와 같다.

1. 하나의 Query에 대해 모든 Key와 dot product를 수행한다.
2. 결과를 $$\sqrt{d_k}$$로 나눈다.
3. Softmax로 모든 Value에 대한 weight를 구한다.
4. Weight와 Value의 dot product를 수행해 출력을 구한다.

하나의 Query에 대해 모든 Key와 연산을 하는 것은 해당 Query와 모든 Key들의 연관성(확률)을 구하는 것이며, 그렇게 해서 얻은 weight와 Value의 연산을 통해 scaling을 수행한다.

Key와 Value는 같은 단어를 의미하지만 Key와의 연산은 각 단어들과의 연관성(확률)을 구하는 것이고 Value와의 연산은 그 확률을 사용해 단어에 대한 attention을 구하는 것으로 볼 수 있다.

위의 과정을 그림으로 표현하면 아래와 같다.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer2.png" alt="">
  <figcaption style="font-size:12px">출처:http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</figcaption>
</figure> 

 한 가지 예를 보도록 하자.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer3.png" alt="">
  <figcaption style="font-size:12px">출처:https://jalammar.github.io/illustrated-transformer/</figcaption>
</figure> 

위의 그림에서 "Thinking"이라는 Query와 모든 단어(여기서는 Thinking과 Machines)의 Key를 곱하고 softmax를 통해 확률 값을 구한다.

그 후에 확률과 모든 단어의 Value를 곱해 weighted Value를 구하고 그 값들을 모두 더해 "Thinking"의 출력 $$z_1$$을 얻는다.

그 후에 $$z_1$$이 또 하나의 sub layer인 position-wise FC feed forward network의 입력으로 들어가게 된다.

위의 그림에서는 "Thinking"이라는 하나의 단어(Query)에 대한 연산만을 계산했지만 실제로는 matrix 형태로 factorization을 한 후에 계산을 한다.

이를 식으로 표현하면 $$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$$이다.

### 4. Multi-head Attention

위에서 소개했던 scaled dot product attention을 하나의 입력 sequence에 대해서 한번에 적용하는 것이 아니라 linear하게 $$h$$번 나눠서 적용한다.

각각의 attention 과정에서 $$d_v$$차원의 출력을 만든 후, 이 $$h$$개의 vector들을 concat해서 새로운 출력을 만든다.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer5.png" alt="">
  <figcaption style="font-size:12px">출처:http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf</figcaption>
</figure> 

이를 식으로 표현하면 아래와 같다.

$$head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$$

$$MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O$$

여기서 $$W$$들은 파라미터이며 각각의 차원은 아래와 같다.

$$W_i^Q,W_i^K: d_{model}\times d_k$$,		$$W_i^V: d_{model}\times d_V$$,		$$W^O: h\times d_v \times d_{model}$$

$$d_{model}$$: transformer 모델의 embedding layer의 출력 vector의 크기

본 논문에서는 $$h=8, d_K=d_V=\frac{d_{model}}{h}=64$$ 로 설정했다.

Multi-head attention의 과정을 정리하면 아래와 같다.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer4.png" alt="">
  <figcaption style="font-size:12px">출처:https://jalammar.github.io/illustrated-transformer/</figcaption>
</figure> 

 Transformer 모델에서는 3가지의 attention이 존재한다.

* **Encoder-decoder attention**

  ​	decoder에 존재하는 attention layer이며 Query가 이전 decoder layer의 출력에서 전달된다.

  ​	Key와 Value는 encoder의 출력에서 전달된다.

  ​	즉, decoder의 각 position에서 encoder의 모든 position의 정보에 관여할 수 있게 만든다.

* **Encoder self attention**

  ​	Query, Key, Value 모두 이전 encoder layer의 출력에서 전달된다.

  ​	즉, encoder의 각 position에서 이전  layer의 출력의 모든 position의 정보에 관여할 수 있게 만든다.

* **Decoder self attention**

  ​	Encoder의 self attention과 마찬가지로 Query, Key, Value 모두 이전 decoder layer의 출력에서 전달된다.

  ​	하지만 encoder와는 다르게 masking을 사용해 미래의 position의 정보에 관여하지 못하도록 만든다.

### 5. Position-wise Feed Forward Network

Transformer 모델에서는 각 position에 동일하게 feed forward network를 적용한다.

두 개의 linear layer가 존재하며 두 layer는 ReLU를 통해 연결한다.

이를 식으로 표현하면, $$FFN(x)=max(0,xW_1+b_1)W_2+b_2$$ 이다.

입력과 출력은 모두 $$d_{model}=512$$차원이며 내부 layer는 $$d_{ff}=2048$$차원을 가진다.

### 6. Embedding & Softmax

Embedding layer에서는 학습된 embedding을 사용해 각 token들을 $$d_{model}$$차원의 vector로 만든다.

Softmax layer에서는 decoder의 출력에 softmax를 적용해 다음 token을 예측한다.

Encoder와 decoder의 embedding 과정과 decoder의 출력을 다음 token을 예측하는 확률로 변환하는 과정에서 모두 같은 matrix를 사용한다.

### 7. Positional Encoding

Transformer 모델은 recurrent 또는 convolution 구조를 사용하지 않기 때문에 sequence의 위치 정보가 필요하다.

이를 위해 encoder와 decoder의 아래에서 positional encoding을 추가한다.

Sequence의 각 position에서 embedding size, 즉 $$d_{model}$$과 같은 크기의 encoding vector를 embedding vector에 더해서 encoder와 decoder의 입력을 구한다.

Positional encoding은 아래와 같다.

$$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$$

$$PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$$

## Why self attention?

지금까지 transformer 모델에 대해 소개했다.

그렇다면 self attention의 장점은 무엇일까?

Self attention을 사용함으로써 아래와 같은 이점을 얻을 수 있다.

* Layer당 전체 연산량이 줄어든다.
* 병렬화가 가능한 연산이 많아진다.
* 거리가 먼 token에 대한 dependency를 잘 학습할 수 있다.

## Flow

Transformer 모델의 전체적인 흐름은 아래 그림과 같이 표현할 수 있다.

<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer6.png" alt="">
  <figcaption style="font-size:12px">출처:https://jalammar.github.io/illustrated-transformer/</figcaption>
</figure> 





<figure class="align-center" style="width:600px">
  <img src="{{ site.url }}{{ site.baseurl }}/assets/images/review/post9/transformer8.png" alt="">
  <figcaption style="font-size:12px">출처:https://jalammar.github.io/illustrated-transformer/</figcaption>
</figure> 

## 논문 링크

### NIPS 2017

[Attention Is All You Need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)

