---
title: "크로스 엔트로피(Cross Entropy)"
categories:
  - 지식창고
tags:
  - Cross Entropy
  - Machine Learning
---

이번 포스팅에서는 딥 러닝 모델의 손실 함수에 사용되는 크로스 엔트로피에 관해 살펴보고자 한다.

## 정보량(Information)

정보이론(Information theory)의 가장 기초가 되는 개념이다.

정보이론은 자주 일어나는 사건보다 드물게 일어나는 사건이 많은 정보량을 가지고 있다는 아이디어에서 시작된다.

예를 들면, 한국에서 어떠한 사람을 찾고 싶은데 그 사람의 성씨라는 정보만 주어졌다고 하자.

그 사람이 '김'씨라면 주어진 정보는 그다지 희소성이 없다.

한국에서 '김'씨 성을 가진 사람들은 엄청 많기 때문이다.

반면에 그 사람이 '맹'씨라면 '김'씨일 때 보다는 희소성이 있다.

'맹'씨 성을 가진 사람은 '김'씨 성을 가진 사람보다 훨씬 적기 때문이다.

다시 말하면, 어떤 사람이 '맹'씨인 사건이 '김'씨인 사건보다 많은 정보량을 가지고 있다고 할 수 있다.

## 엔트로피(Entropy)

확률변수 $$X$$의 값이 $$x$$인 사건의 정보량 $$I(x)$$는 아래와 같다.

$$I(x) = -logP(x)$$

예를 들어, 동전을 던져 앞면이 나오는 사건과 주사위를 던져 6이 나오는 사건을 비교해보자.

전자의 정보량은 $$-log_2{0.5} = 1$$이고, 후자의 정보량은 $$-log_2{\frac{1}{6}} = 2.58$$이다.

즉, 전자의 사건보다 후자의 사건이 더 많은 정보를 가지고 있다고 볼 수 있다.

엔트로피는 간단하게 말하면 사건 정보량의 기대값이다.

어떤 확률분포 $$P$$에 대한 엔트로피 $$H(P)$$는 아래와 같다.

$$H(P) = H(x) = E_{X\sim P}{[I(x)]} = -E_{X\sim P}{[logP(x)]}$$

위의 식을 아래와 같이 표현할 수 있다.

$$H(P) = H(x) = -\sum_x{P(x)logP(x)}$$

## KL Divergence(Kullback-Leibler Divergence)

KL Divergence는 두 확률분포의 차이를 계산할 때 사용한다.

딥 러닝 모델의 경우는 우리가 가지고 있는 데이터의 분포 $$P(x)$$와 모델이 예측한 데이터의 분포 $$Q(x)$$ 사이의 차이를 계산할 때 사용할 수 있다.

두 확률분포 $$P(x)$$와 $$Q(x)$$의 KL Divergence는 아래와 같다.

$$D_{KL}{(P||Q)} = E_{X\sim P}{[log\frac{P(x)}{Q(x)}]} = E_{X\sim P}[logP(x)-logQ(x)]$$

또는 아래와 같이 표현할 수 있다.

$$D_{KL}{(P||Q)} = H(P,Q) - H(P) = (-\sum_x{P(x)logQ(x)}) - (-\sum_x{P(x)logP(x)})$$

주의할 점은 KL Divergence는 대칭적인 함수가 아니기 때문에 P와 Q의 순서가 바뀌면 그 값도 달라진다는 것이다.

## 크로스 엔트로피(Cross Entropy)

위의 식에서도 보았듯이 크로스 엔트로피와 KL Divergence의 관계는 아래와 같다.

$$H(P,Q) = H(P) + D_{KL}(P||Q)$$

위의 식을 아래와 같이 표현할 수 있다.

$$H(P,Q) = -E_{X\sim P}[logQ(x)] = -\sum_x{P(x)logQ(x)}$$

딥 러닝에서는 학습 데이터의 분포 $$P(x)$$와 모델이 예측한 데이터의 분포 $$Q(x)$$의 크로스 엔트로피 $$H(P,Q)$$를 최소화하는 방향으로 모델을 학습시킬 수 있다.

$$Q(x)$$가 $$P(x)$$에 가까워질 수록 $$-\sum_x{P(x)logQ(x)}$$, 즉, 크로스 엔트로피 $$H(P,Q)$$가 감소한다.

다시 말하면, 크로스 엔트로피를 최소화한다는 의미는 모델의 예측 결과 분포 $$Q(x)$$를 학습 데이터 분포 $$P(x)$$에 가깝도록 만든다는 것으로 볼 수 있다.

딥 러닝 모델을 학습할 때, 특히 class가 여러개인 classifier를 학습할 때 softmax 함수와 크로스 엔트로피를 사용해 손실 함수를 정의하는 경우가 자주 있다.

이런 classifier에서는 학습 데이터의 label이 [1 0 0 0] 같은 one-hot vector이고 softmax를 거친 모델의 예측값은 [0.7 0.1 0.05 0.15] 같은 vector일 것이다.

여기서 크로스 엔트로피를 최소화 한다는 것은 [0.7 0.1 0.05 0.15]가 [1 0 0 0]에 가까워지록 모델의 파라미터를 학습한다고 의미로 볼 수 있다.
